{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpudb\n",
    "from arcgis import GIS\n",
    "from arcgis import geometry\n",
    "import json\n",
    "import csv\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dte_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# kinetica \n",
    "kinetica_cred_file = '.env'\n",
    "kinetica_host = 'admin.citizenscience.host'\n",
    "\n",
    "# kinetica table query options\n",
    "table_name = 'plastics_db'\n",
    "limit = 1000000\n",
    "encoding = 'json'\n",
    "query_options = {\n",
    "    'sort_by': 'captured_time',\n",
    "    'sort_order': 'DESC'\n",
    "}\n",
    "\n",
    "# use for testing\n",
    "# ex: '2020-02-02 00:00:00'\n",
    "QUERY_TIMESTAMP_OVERRIDE = '2020-03-05 16:00:48'\n",
    "\n",
    "dte_now = datetime.now(tz=pytz.timezone('GMT'))\n",
    "if QUERY_TIMESTAMP_OVERRIDE:\n",
    "    dte_now = datetime.strptime(QUERY_TIMESTAMP_OVERRIDE, dte_format)\n",
    "    \n",
    "query_timestamp_now = datetime.strftime(dte_now, dte_format)\n",
    "\n",
    "dte_past = dte_now - timedelta(minutes=15)\n",
    "query_timestamp_past = datetime.strftime(dte_past, dte_format)\n",
    "\n",
    "# print (query_timestamp_past, query_timestamp_now)\n",
    "\n",
    "log_file = f'logs/{query_timestamp_now.replace(\":\", \"\")}.csv'\n",
    "\n",
    "query_options['expression'] = f'captured_time > \\'{query_timestamp_past}\\''\n",
    "\n",
    "# ArcGIS Online config options\n",
    "conn_profile_id = 'ago_ec2020_py'\n",
    "plastics_layer_id = '08878e5ab81d4074932a1069db4ded75'\n",
    "plastics_perimeter_layer_id = '5c952389060d4f199931f7c0622541bc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create log file\n",
    "# log_filename = f'{query_timestamp_now.replace(\":\", \"\")}.csv'\n",
    "with open(log_file, 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['datetime', 'message'])\n",
    "    \n",
    "def log_message(message):\n",
    "    with open(log_file, 'a+') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        dte = datetime.now(tz=pytz.timezone('GMT'))\n",
    "        log_ts = datetime.strftime(dte,dte_format)\n",
    "        \n",
    "        csv_writer.writerow([log_ts, message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "### Parse Inventory Data for each database record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inventory_data(recordID, inv_data, cleanup_workflow):    \n",
    "    inv_data_atts = {}\n",
    "    try:\n",
    "        inv_data_json = json.loads(inv_data)\n",
    "    except:\n",
    "        msg = f'unable to parse inventory_data values as JSON for recordID :: {recordID}'\n",
    "        print(msg)\n",
    "        log_message(msg)\n",
    "        return inv_data_atts\n",
    "    \n",
    "    # if cleanup_workflow is None; user selected \"wander\" in the app\n",
    "    if cleanup_workflow == 'sample' or cleanup_workflow is None:\n",
    "        for att in inv_data_json:\n",
    "            for child in att['childrens']:\n",
    "                field_name = f\"sample_{att['id']}_{child['id']}\"\n",
    "                inv_data_atts[field_name] = child['value']\n",
    "    elif cleanup_workflow == 'report':\n",
    "        for att in inv_data_json:\n",
    "            in_name = att['type'].replace(' ', '')\n",
    "            in_name = in_name[0].lower() + in_name[1:]\n",
    "            field_name = f'report_{in_name}'\n",
    "            inv_data_atts[field_name] = att['value']\n",
    "        \n",
    "    return inv_data_atts    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and re-project geometry of collection point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_geometry(lat, lng):\n",
    "    geom = None\n",
    "    try:\n",
    "        geom = geometry.project([{'x': lng, 'y': lat, 'spatialReference': {'wkid': 4326}}], in_sr=4326, out_sr=3857)[0]\n",
    "    except:\n",
    "        msg = f'unable to project geometry of collection point for recordID :: {feature[\"attributes\"][\"recordID\"]}'\n",
    "        print(msg)\n",
    "        log_message(msg)\n",
    "        \n",
    "    return geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and re-project geometry for perimeter feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_perimeter_feature(attributes, perimeter_json):\n",
    "    feature = {\n",
    "        'attributes': {},\n",
    "        'geometry': None\n",
    "    }\n",
    "    \n",
    "    # copy observation attributes into perimeter attributes\n",
    "    for att in attributes:\n",
    "        feature['attributes'][att] = attributes[att]\n",
    "    \n",
    "    # project geographic coords to web mercator\n",
    "    geom = None\n",
    "    try:\n",
    "        geom = geometry.project([perimeter_json], in_sr=4326, out_sr=3857)[0]\n",
    "        feature['geometry'] = geom\n",
    "    except:\n",
    "        msg = f'unable to project geometry of perimeter geometry for recordID :: {feature[\"attributes\"][\"recordID\"]}'\n",
    "        print(msg)\n",
    "        log_message(msg)\n",
    "            \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break up an array into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_it(in_list, chunk_size):\n",
    "    chunked = [in_list[i * chunk_size:(i + 1) * chunk_size] for i in range((len(in_list) + chunk_size - 1) // chunk_size )] \n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Kinetica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = None\n",
    "pw = None\n",
    "with open(kinetica_cred_file) as f:\n",
    "    config = json.loads(f.read())\n",
    "    user = config['user']\n",
    "    pw = config['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_db = gpudb.GPUdb(\n",
    "    host=kinetica_host,\n",
    "    username=user,\n",
    "    password=pw\n",
    ")\n",
    "msg = f'connected to kinetica db @ {kinetica_host}'\n",
    "print (msg)\n",
    "log_message(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Plastics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmsg = f'querying kinetica :: {query_options[\"expression\"]}'\n",
    "print (qmsg)\n",
    "log_message(qmsg)\n",
    "\n",
    "recs = h_db.get_records(table_name=table_name, limit=limit, encoding=encoding, options=query_options)\n",
    "\n",
    "query_record_count = recs['total_number_of_records']\n",
    "resmsg = f'({query_record_count}) records returned from query'\n",
    "print (resmsg)\n",
    "log_message(resmsg)\n",
    "\n",
    "if query_record_count == 0:\n",
    "    msg = 'no records returned from Kinetica. exiting ...'\n",
    "    log_message(msg)\n",
    "    raise SystemExit(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to ArcGIS Online and setup feature layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgis = None\n",
    "try:\n",
    "    ecgis = GIS(profile=conn_profile_id)\n",
    "except:\n",
    "    msg = f'unable to connect to ec2020 ArcGIS Online Org'\n",
    "    print(msg)\n",
    "    log_message(msg)\n",
    "    raise SystemExit(msg)\n",
    "\n",
    "plastics_layer = None\n",
    "plastics_perimeter_layer = None\n",
    "try:\n",
    "    plastics_layer = ecgis.content.get(plastics_layer_id).layers[0]\n",
    "    plastics_perimeter_layer = ecgis.content.get(plastics_perimeter_layer_id).layers[0]\n",
    "except:\n",
    "    msg = f'unable to connect to feature layer(s)'\n",
    "    print(msg)\n",
    "    log_message(msg)\n",
    "    raise SystemExit(msg)\n",
    "\n",
    "msg = 'successfully connected to EC2020 ArcGIS Online and connected to feature layer(s)'\n",
    "print(msg)\n",
    "log_message(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each record from kinetica, create and stage features to add to feature services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds = []\n",
    "adds_attachments = {}\n",
    "perimeter_adds = []\n",
    "msg = 'processing query results ...'\n",
    "print (msg)\n",
    "log_message(msg)\n",
    "\n",
    "for rec in recs['records_json']:\n",
    "    feature = {\n",
    "        'attributes': {},\n",
    "        'geometry': None\n",
    "    }\n",
    "    \n",
    "    # load the record as a JSON object\n",
    "    rec_json = json.loads(rec)\n",
    "    \n",
    "    # recordID\n",
    "    recordID = rec_json['recordID']\n",
    "    \n",
    "    # individualID\n",
    "    individualID = rec_json['individualID']\n",
    "    \n",
    "    # appInstallID\n",
    "    appInstallID = rec_json['appInstallID']\n",
    "    \n",
    "    # captured_time\n",
    "    captured_time = rec_json['captured_time']\n",
    "    \n",
    "    # submitted_time\n",
    "    submitted_time = rec_json['submitted_time']\n",
    "    \n",
    "    # image_filepath\n",
    "    image_filepath = rec_json['image_filepath'] if rec_json['image_filepath'] else None\n",
    "\n",
    "    # adding attachments from URLs won't work. only from local disk\n",
    "    # we'd have to download each image locally, then reference it in the attachment upload, then delete after\n",
    "    # high LOE, not much payoff. leave sample code in for now\n",
    "    \n",
    "#     if image_filepath is not None:\n",
    "#         adds_attachments[recordID] = image_filepath\n",
    "           \n",
    "    # action_taken\n",
    "    action_taken = rec_json['action_taken']\n",
    "    \n",
    "    # plastics_mode\n",
    "    plastics_mode = rec_json['plastics_mode']\n",
    "    \n",
    "    # cleanup_workflow\n",
    "    cleanup_workflow = rec_json['cleanup_workflow']\n",
    "    \n",
    "    # inventory_data\n",
    "    inv_data_atts = {}\n",
    "    if not rec_json['inventory_data'] and not rec_json['inventory_data_pct']:\n",
    "        msg = f'no values in either inventory_data or inventory_data_pct for recordID :: {recordID}'\n",
    "        print (msg)\n",
    "        log_message(msg)\n",
    "    else:\n",
    "        if cleanup_workflow == 'report':\n",
    "            inventory_data = rec_json['inventory_data_pct']\n",
    "        else:\n",
    "            inventory_data = rec_json['inventory_data']\n",
    "    \n",
    "        inv_data_atts = parse_inventory_data(recordID, inventory_data, cleanup_workflow)\n",
    "    \n",
    "    # report_workflow\n",
    "    report_workflow = rec_json['report_workflow']\n",
    "    \n",
    "    # polygon_perimeter\n",
    "    polygon_perimeter = None\n",
    "    try:\n",
    "        polygon_perimeter = json.loads(rec_json['polygon_perimeter'])\n",
    "    except:\n",
    "        print (f'no perimeter found for recordID: {recordID}')\n",
    "    \n",
    "    # cleanup_zone_status\n",
    "    cleanup_zone_status = rec_json['cleanup_zone_status']\n",
    "    \n",
    "    # terrain\n",
    "    terrain = rec_json['terrain']\n",
    "    \n",
    "    # bag_count\n",
    "    bag_count_json = None\n",
    "    bag_count_sm = None\n",
    "    bag_count_md = None\n",
    "    bag_count_lg = None\n",
    "    if rec_json['bag_count']:\n",
    "        bag_count_json = json.loads(rec_json['bag_count'])\n",
    "        bag_count_sm = int(bag_count_json['small'])\n",
    "        bag_count_md = int(bag_count_json['medium'])\n",
    "        bag_count_lg = int(bag_count_json['large'])\n",
    "    \n",
    "    # lat\n",
    "    lat = None\n",
    "    if rec_json['lat'] is not None:\n",
    "        lat = float(rec_json['lat'])\n",
    "    \n",
    "    # lng\n",
    "    lng = None\n",
    "    if rec_json['long'] is not None:\n",
    "        lng = float(rec_json['long'])\n",
    "    \n",
    "    # cleanup_event_id\n",
    "    cleanup_event_id = int(rec_json['cleanup_event_id']) if rec_json['cleanup_event_id'] else None\n",
    "    \n",
    "    # cleanup_zone_id\n",
    "    cleanup_zone_id = int(rec_json['cleanup_zone_id']) if rec_json['cleanup_zone_id'] else None\n",
    "    \n",
    "    # cleanup_grid_id\n",
    "    cleanup_grid_id = int(rec_json['cleanup_grid_id']) if rec_json['cleanup_grid_id'] else None\n",
    "\n",
    "    feature['attributes'] = {\n",
    "        'recordID': recordID,\n",
    "        'individualID': individualID,\n",
    "        'appInstallID': appInstallID,\n",
    "        'captured_time': captured_time,\n",
    "        'submitted_time': submitted_time,\n",
    "        'image_filepath': image_filepath,\n",
    "        'action_taken': action_taken,\n",
    "        'plastics_mode': plastics_mode,\n",
    "        'cleanup_workflow': cleanup_workflow,\n",
    "        'report_workflow': report_workflow,\n",
    "        'cleanup_zone_status': cleanup_zone_status,\n",
    "        'terrain': terrain,\n",
    "        'bag_count_sm': bag_count_sm,\n",
    "        'bag_count_md': bag_count_md,\n",
    "        'bag_count_lg': bag_count_lg,\n",
    "        'lat': lat,\n",
    "        'lng': lng,\n",
    "        'cleanup_event_id': cleanup_event_id,\n",
    "        'cleanup_zone_id': cleanup_zone_id,\n",
    "        'cleanup_grid_id': cleanup_grid_id\n",
    "    }\n",
    "    \n",
    "    # combine inventory_data with base feature\n",
    "    for att in inv_data_atts:\n",
    "        feature['attributes'][att] = inv_data_atts[att]\n",
    "    \n",
    "    feature['geometry'] = create_feature_geometry(lat, lng)\n",
    "    \n",
    "    adds.append(feature)\n",
    "    \n",
    "    # create associated perimeter feature and stage\n",
    "    if polygon_perimeter is not None:\n",
    "        perimeter_feature = create_perimeter_feature(feature['attributes'], polygon_perimeter)\n",
    "        perimeter_adds.append(perimeter_feature)\n",
    "\n",
    "msg = f'done processing query results. ({len(adds)}) collection features to add and ({len(perimeter_adds)}) perimeter features to add.'\n",
    "print (msg)\n",
    "log_message(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk up features into batches of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'chunking features into chunks of 1000 ...'\n",
    "print (msg)\n",
    "log_message(msg)\n",
    "\n",
    "feature_chunks = chunk_it(adds, 1000)\n",
    "perim_chunks = chunk_it(perimeter_adds, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send updates to Plastics feature service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'adding features to plastics db ...'\n",
    "print (msg)\n",
    "log_message(msg)\n",
    "\n",
    "chunk_length = len(feature_chunks)\n",
    "for i, chunk in enumerate(feature_chunks):\n",
    "    try:\n",
    "        res = plastics_layer.edit_features(adds=chunk)\n",
    "        added = len(res['addResults'])\n",
    "        msg = f'({added}) features were added'\n",
    "        print (msg)\n",
    "        log_message(msg)\n",
    "    except Exception as e:    \n",
    "        msg = f'error adding chunk {i+1} of {chunk_length} to plastics layer :: {e}'\n",
    "        print (msg)\n",
    "        log_message(msg)\n",
    "\n",
    "msg = 'done adding features to plastics layer'\n",
    "print (msg)\n",
    "log_message(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send updates to Perimeter feature service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'adding features to plastics perimeter layer db ...'\n",
    "print (msg)\n",
    "log_message(msg)\n",
    "\n",
    "chunk_length = len(perim_chunks)\n",
    "for i, chunk in enumerate(perim_chunks):\n",
    "    try:\n",
    "        res = plastics_perimeter_layer.edit_features(adds=chunk)\n",
    "        added = len(res['addResults'])\n",
    "        msg = f'({added}) features were added'\n",
    "        print (msg)\n",
    "        log_message(msg)\n",
    "    except Exception as e:    \n",
    "        msg = f'error adding chunk {i+1} of {chunk_length} to plastics perimeter layer :: {e}'\n",
    "        print (msg)\n",
    "        log_message(msg)\n",
    "\n",
    "msg = 'done adding features to plastics perimeter layer'\n",
    "print (msg)\n",
    "log_message(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = 'script completed'\n",
    "print (msg)\n",
    "log_message(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
