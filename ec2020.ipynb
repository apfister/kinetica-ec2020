{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpudb\n",
    "from arcgis import GIS\n",
    "from arcgis import geometry\n",
    "import json\n",
    "import csv\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dte_format = '%Y-%m-%d %H:%M:%S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dte = datetime.now(tz=pytz.timezone('GMT')) - timedelta(days=1)\n",
    "query_timestamp = datetime.strftime(dte,dte_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create log file\n",
    "log_filename = f'{query_timestamp.replace(\":\", \"\")}.csv'\n",
    "with open(log_filename, 'w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['datetime', 'message'])\n",
    "    \n",
    "def log_message(message):\n",
    "    with open(log_filename, 'a+') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        dte = datetime.now(tz=pytz.timezone('GMT')) - timedelta(days=1)\n",
    "        log_ts = datetime.strftime(dte,dte_format)\n",
    "        \n",
    "        csv_writer.writerow([log_ts, message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get last run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run = None\n",
    "with open('last-run.txt') as txt_contents:\n",
    "    txt = txt_contents.read()\n",
    "    last_run = datetime.strptime(txt, dte_format)\n",
    "\n",
    "if last_run is None:\n",
    "    msg = 'unable to get last run. exiting ...'\n",
    "    log_message(msg)\n",
    "    raise SystemExit(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Kinetica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_db = gpudb.GPUdb(\n",
    "    host=\"admin.citizenscience.host\",\n",
    "    username=\"apfister\",\n",
    "    password=\"iswk&01Wn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kinetica query options\n",
    "Use expression to only get records since the last time this script ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'plastics_db'\n",
    "limit = 1000000\n",
    "encoding = 'json'\n",
    "options = {\n",
    "    'expression': f'captured_time > \\'{last_run}\\'',\n",
    "    'sort_by': 'captured_time',\n",
    "    'sort_order': 'DESC'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Plastics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message(f'querying kinetica :: {options[\"expression\"]}')\n",
    "\n",
    "recs = h_db.get_records(table_name=table_name, limit=limit, encoding=encoding, options=options)\n",
    "\n",
    "query_record_count = recs['total_number_of_records']\n",
    "\n",
    "log_message(f'({query_record_count}) records returned from query')\n",
    "\n",
    "if query_record_count == 0:\n",
    "    msg = 'no records returned from Kinetica'\n",
    "    log_message(msg)\n",
    "    raise SystemExit(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to ArcGIS Online and setup feature layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgis = GIS(profile='ago_ec2020_py')\n",
    "plastics_layer = ecgis.content.get('08878e5ab81d4074932a1069db4ded75').layers[0]\n",
    "plastics_perimeter_layer = ecgis.content.get('5c952389060d4f199931f7c0622541bc').layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Inventory Data for each database record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inventory_data(inv_data, cleanup_workflow):    \n",
    "    inv_data_atts = {}\n",
    "    try:\n",
    "        inv_data_json = json.loads(inv_data)\n",
    "    except:\n",
    "        print ('unable to parse inventory_data')\n",
    "        return inv_data_atts\n",
    "    \n",
    "    if cleanup_workflow == 'sample':\n",
    "        for att in inv_data_json:\n",
    "            for child in att['childrens']:\n",
    "                field_name = f\"sample_{att['id']}_{child['id']}\"\n",
    "                inv_data_atts[field_name] = child['value']\n",
    "    elif cleanup_workflow == 'report':\n",
    "        for att in inv_data_json:\n",
    "            in_name = att['type'].replace(' ', '')\n",
    "            in_name = in_name[0].lower() + in_name[1:]\n",
    "            field_name = f'report_{in_name}'\n",
    "            inv_data_atts[field_name] = att['value']\n",
    "        \n",
    "    return inv_data_atts    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and re-project geometry of collection point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_geometry(lat, lng):\n",
    "    geom = None\n",
    "    try:\n",
    "        geom = geometry.project([{'x': lng, 'y': lat, 'spatialReference': {'wkid': 4326}}], in_sr=4326, out_sr=3857)[0]\n",
    "    except:\n",
    "        print ('unable to project geometry of collection point')\n",
    "        \n",
    "    return geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and re-project geometry for perimeter feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_perimeter_feature(attributes, perimeter_json):\n",
    "    feature = {\n",
    "        'attributes': {},\n",
    "        'geometry': None\n",
    "    }\n",
    "    \n",
    "    # copy observation attributes into perimeter attributes\n",
    "    for att in attributes:\n",
    "        feature['attributes'][att] = attributes[att]\n",
    "    \n",
    "    # project geographic coords to web mercator\n",
    "    geom = None\n",
    "    try:\n",
    "        geom = geometry.project([perimeter_json], in_sr=4326, out_sr=3857)[0]\n",
    "        feature['geometry'] = geom\n",
    "    except:\n",
    "        print ('unable to project geometry of collection point')\n",
    "            \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each record from kinetica, create and stage features to add to feature services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds = []\n",
    "adds_attachments = {}\n",
    "perimeter_adds = []\n",
    "print ('processing query results ...')\n",
    "for rec in recs['records_json']:\n",
    "    feature = {\n",
    "        'attributes': {},\n",
    "        'geometry': None\n",
    "    }\n",
    "    \n",
    "    # load the record as a JSON object\n",
    "    rec_json = json.loads(rec)\n",
    "    \n",
    "    # recordID\n",
    "    recordID = rec_json['recordID']\n",
    "    \n",
    "    # individualID\n",
    "    individualID = rec_json['individualID']\n",
    "    \n",
    "    # appInstallID\n",
    "    appInstallID = rec_json['appInstallID']\n",
    "    \n",
    "    # captured_time\n",
    "    cp_str = rec_json['captured_time']\n",
    "#     cp_time = datetime.strptime(cp_str, dte_format)\n",
    "    # print (cp_str, ' -- ', cp_time)\n",
    "    \n",
    "    # submitted_time\n",
    "    sub_str = rec_json['submitted_time']\n",
    "#     sub_time = datetime.strptime(sub_str, dte_format)\n",
    "    # print (sub_str, ' -- ', sub_time)\n",
    "    \n",
    "    # image_filepath\n",
    "    image_filepath = rec_json['image_filepath'] if rec_json['image_filepath'] else None\n",
    "\n",
    "    # adding attachments from URLs won't work. only from local disk\n",
    "    # we'd have to download each image locally, then reference it in the attachment upload, then delete after\n",
    "    # high LOE, not much payoff. leave sample code in for now\n",
    "    \n",
    "#     if image_filepath is not None:\n",
    "#         adds_attachments[recordID] = image_filepath\n",
    "           \n",
    "    # action_taken\n",
    "    action_taken = rec_json['action_taken']\n",
    "    \n",
    "    # plastics_mode\n",
    "    plastics_mode = rec_json['plastics_mode']\n",
    "    \n",
    "    # cleanup_workflow\n",
    "    cleanup_workflow = rec_json['cleanup_workflow']\n",
    "    \n",
    "    # inventory_data\n",
    "    inventory_data = None\n",
    "    if cleanup_workflow == 'report':\n",
    "        inventory_data = rec_json['inventory_data_pct']\n",
    "    else:\n",
    "        inventory_data = rec_json['inventory_data']\n",
    "    \n",
    "    inv_data_atts = parse_inventory_data(inventory_data, cleanup_workflow)\n",
    "    \n",
    "    # report_workflow\n",
    "    report_workflow = rec_json['report_workflow']\n",
    "    \n",
    "    # polygon_perimeter\n",
    "    polygon_perimeter = None\n",
    "    try:\n",
    "        polygon_perimeter = json.loads(rec_json['polygon_perimeter'])\n",
    "    except:\n",
    "        print (f'no perimeter found for recordID: {recordID}')\n",
    "    \n",
    "    # cleanup_zone_status\n",
    "    cleanup_zone_status = rec_json['cleanup_zone_status']\n",
    "    \n",
    "    # terrain\n",
    "    terrain = rec_json['terrain']\n",
    "    \n",
    "    # bag_count\n",
    "    bag_count_json = None\n",
    "    bag_count_sm = None\n",
    "    bag_count_md = None\n",
    "    bag_count_lg = None\n",
    "    if rec_json['bag_count']:\n",
    "        bag_count_json = json.loads(rec_json['bag_count'])\n",
    "        bag_count_sm = int(bag_count_json['small'])\n",
    "        bag_count_md = int(bag_count_json['medium'])\n",
    "        bag_count_lg = int(bag_count_json['large'])\n",
    "    \n",
    "    # lat\n",
    "    lat = None\n",
    "    if rec_json['lat'] is not None:\n",
    "        lat = float(rec_json['lat'])\n",
    "    \n",
    "    # lng\n",
    "    lng = None\n",
    "    if rec_json['long'] is not None:\n",
    "        lng = float(rec_json['long'])\n",
    "    \n",
    "    # cleanup_event_id\n",
    "    cleanup_event_id = int(rec_json['cleanup_event_id']) if rec_json['cleanup_event_id'] else None\n",
    "    \n",
    "    # cleanup_zone_id\n",
    "    cleanup_zone_id = int(rec_json['cleanup_zone_id']) if rec_json['cleanup_zone_id'] else None\n",
    "    \n",
    "    # cleanup_grid_id\n",
    "    cleanup_grid_id = int(rec_json['cleanup_grid_id']) if rec_json['cleanup_grid_id'] else None\n",
    "\n",
    "    feature['attributes'] = {\n",
    "        'recordID': recordID,\n",
    "        'individualID': individualID,\n",
    "        'appInstallID': appInstallID,\n",
    "        'cp_time': cp_str,\n",
    "        'sub_time': sub_str,\n",
    "        'image_filepath': image_filepath,\n",
    "        'action_taken': action_taken,\n",
    "        'plastics_mode': plastics_mode,\n",
    "        'cleanup_workflow': cleanup_workflow,\n",
    "        'report_workflow': report_workflow,\n",
    "        'cleanup_zone_status': cleanup_zone_status,\n",
    "        'terrain': terrain,\n",
    "        'bag_count_sm': bag_count_sm,\n",
    "        'bag_count_md': bag_count_md,\n",
    "        'bag_count_lg': bag_count_lg,\n",
    "        'lat': lat,\n",
    "        'lng': lng,\n",
    "        'cleanup_event_id': cleanup_event_id,\n",
    "        'cleanup_zone_id': cleanup_zone_id,\n",
    "        'cleanup_grid_id': cleanup_grid_id\n",
    "    }\n",
    "    \n",
    "    # combine inventory_data with base feature\n",
    "    for att in inv_data_atts:\n",
    "        feature['attributes'][att] = inv_data_atts[att]\n",
    "    \n",
    "    feature['geometry'] = create_feature_geometry(lat, lng)\n",
    "    \n",
    "    adds.append(feature)\n",
    "    \n",
    "    # create associated perimeter feature and stage\n",
    "    if polygon_perimeter is not None:\n",
    "        perimeter_feature = create_perimeter_feature(feature['attributes'], polygon_perimeter)\n",
    "        perimeter_adds.append(perimeter_feature)\n",
    "\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk up features into batches of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_it(in_list, chunk_size):\n",
    "    chunked = [in_list[i * chunk_size:(i + 1) * chunk_size] for i in range((len(in_list) + chunk_size - 1) // chunk_size )] \n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send updates to Plastics feature service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_chunks = chunk_it(adds, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = len(feature_chunks)\n",
    "for i, chunk in enumerate(feature_chunks):\n",
    "    try:\n",
    "        res = plastics_layer.edit_features(adds=chunk)\n",
    "        success_msg = f'added chunk {i+1} of {chunk_length} to plastics_layer'\n",
    "        log_message(success_msg)\n",
    "    except Exception as e:    \n",
    "        err_msg = f'error adding chunk to plastics layer :: {e}'\n",
    "        log_message(err_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send updates to Perimeter feature service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perim_chunks = chunk_it(perimeter_adds, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = len(perim_chunks)\n",
    "for i, chunk in enumerate(perim_chunks):\n",
    "    try:\n",
    "        plastics_perimeter_layer.edit_features(adds=chunk)\n",
    "        success_msg = f'added chunk {i+1} of {chunk_length} to plastics_perimeter_layer'\n",
    "        log_message(success_msg)\n",
    "    except Exception as e:    \n",
    "        err_msg = f'error adding chunk to plastics layer :: {e}'\n",
    "        log_message(err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_message('script completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
